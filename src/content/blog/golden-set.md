---
title: "Golden set + evals w praktyce: jak testowa LLM-y tak, 偶eby to naprawd dziaao"
date: 2026-02-25
excerpt: Pora poczy niedeterministyczno i oceny w proces, kt贸ry ma sens. Dowiedz si, jak zbudowa i wykorzysta Golden Set w testowaniu AI.
draft: false
lang: pl
tags: ["LLM", "QA", "Evals", "Golden Set"]
---

W pierwszej czci m贸wilimy o tym, 偶e LLM-y s niedeterministyczne. W drugiej - 偶e klasyczne "pass/fail" nie dziaa i musimy ocenia r贸偶ne aspekty odpowiedzi (evals).

W trzeciej pora poczy to w proces, kt贸ry odpowiada na najwa偶niejsze pytanie w QA: **"Skd mam wiedzie, czy po zmianie promptu lub modelu system faktycznie dziaa lepiej?"**.

Odpowiedzi jest duet: golden set + evals. I spokojnie - jeli do tej pory pracowaa/pracowae g贸wnie z testami API czy mobilk, to przejcie w wiat AI wcale nie jest takie straszne.

## Czym jest golden set?

Golden set to zestaw wej (prompt贸w) i wzorcowych, zaakceptowanych przez ludzi odpowiedzi, kt贸re su偶 jako punkt odniesienia (ground truth) przy testowaniu modelu. To Twoja "zota biblioteka" przypadk贸w testowych.

Tyle 偶e w wiecie LLM oczekiwany rezultat nie jest sztywnym szablonem, tylko zbiorem informacji:

- **Wzorcow odpowiedzi**, kt贸ra pokazuje idealny ton i struktur.
- **Komentarzem eksperta**, kt贸ry wyjania, dlaczego ta odpowied藕 jest dobra.
- **Kryteriami jakoci (evals)**, kt贸re pozwalaj zmierzy stopie realizacji zadania.

Mo偶esz o tym myle jak o "skalowaniu wiedzy eksperckiej". Raz decydujesz, jak brzmi sukces, a automatyzacja pilnuje tego za Ciebie przy ka偶dej kolejnej zmianie.

## Po co nam golden set, skoro odpowiedzi mog by r贸偶ne?

Bo r贸偶ne odpowiedzi te偶 mog by dobre. Golden set nie udaje deterministycznoci. On robi co znacznie wa偶niejszego: **wyznacza granice akceptowalnoci**.

1. **Definiuje sukces**: To Tw贸j "Expected Result", ale w wersji elastycznej. Wyznacza standard, kt贸rego oczekujesz od systemu.
2. **Umo偶liwia ocen przez por贸wnanie (Reference-based)**: To kluczowy koncept. Ten, kto ocenia (czowiek lub w przyszoci inny model AI), nie sprawdza tekstu 1:1. U偶ywa Twojego wzorca jako semantycznego drogowskazu. Wie, 偶e odpowied藕 powinna zawiera te same fakty i ton, co wzorzec, nawet jeli u偶ywa innych s贸w.
3. **Chroni przed regresj**: To Tw贸j g贸wny bezpiecznik. Gdy aktualizujesz model lub zmieniasz prompt, Golden Set m贸wi Ci od razu: "uwaga, jako merytoryczna spada". Bez tego dziaasz po omacku.

To troch jak zdjcie w dowodzie: ka偶dy z nas wyglda codziennie troch inaczej, ale wci偶 jest t sam osob. Golden set m贸wi nam: "to wci偶 jest akceptowalna odpowied藕".

## Z czego skada si dobry golden set?

Najlepiej, jeli ka偶dy przykad w golden secie zawiera cztery kluczowe elementy:

### 1. Prompt

Czyli to, co u偶ytkownik wpisze. Nie "adne prompt engineering", tylko realny input: kr贸tki, krzywy, dwuznaczny, 藕le sformuowany, a czasem absurdalny.

Golden set powinien odwzorowywa realne zachowania u偶ytkownik贸w - a ci nie pisz idealnych prompt贸w. (Ja te偶 nie).

### 2. Oczekiwana dobra odpowied藕 (golden answer)

To nie musi by jedyny suszny tekst. To mo偶e by:

- wzorcowa odpowied藕 zdefiniowana przez czowieka,
- kilka przykad贸w r贸wnowa偶nych odpowiedzi,
- szkic + komentarz ("odpowied藕 powinna zawiera A, B, unika C").

### 3. Ocena wedug kategorii evals

Tu pojawia si magia. Golden set nie jest tylko zbiorem odpowiedzi, ale te偶 ich ocen wedug kategorii (pamitasz poprzedni wpis?):

- **Fidelity** (czy wykonuje zadanie),
- **Relevance** (czy trzyma si tematu),
- **Safety** (czy jest bezpieczna),
- **Tone** (czy brzmi jak trzeba),
- **Context** (czy wykorzystuje kontekst).

Golden answer zawsze ma wysok ocen - to nasz wzorzec.

### 4. Komentarze QA

Najwa偶niejszy element, o kt贸rym wiele os贸b zapomina. Dobry golden set zawiera notatki:

- dlaczego ta odpowied藕 jest poprawna,
- co byoby bdem,
- gdzie s granice tolerancji (np. "mo偶e by inaczej sformuowane, ale musi zawiera X").

To opisuje "intencj QA", kt贸ra potem pomaga w automatyzacji.

## Jak golden set wsp贸pracuje z evals?

Najprociej: Golden set to Twoje "paliwo" (przykady), a evals to Twoja "procedura kontrolna" (spos贸b oceniania).

W nowoczesnym QA zapominamy o prostym por贸wnywaniu tekstu. Zamiast tego wdra偶amy proces:

1. Dajesz modelowi prompt z golden setu.
2. Model generuje odpowied藕.
3. **Osoba lub system oceniajcy dostaje Twoj "zot odpowied藕" jako referencj.**
4. **Ten, kto sprawdza wynik, nie liczy przecink贸w.** Sprawdza, czy intencja i fakty zgadzaj si z Twoim wzorcem.

To dlatego oceny oparte o referencje s tak skuteczne - pozwalaj zachowa elastyczno AI, nie tracc kontroli nad jakoci.

## Jak zbudowa golden set krok po kroku?

### Krok 1: Zbierz realne przypadki

Nie wymylaj - obserwuj: zgoszenia od u偶ytkownik贸w, logi czatu, bdne odpowiedzi modelu, edge-case'y. To najcenniejsze 藕r贸do wiedzy.

### Krok 2: Przepisz je w form "prompt -> odpowied藕"

Zadbaj o to, aby odpowied藕 bya wysokiej jakoci, kompletna, nie halucynowaa i bya zgodna z politykami bezpieczestwa. Pamitaj: golden answer powinna by lepsza ni偶 typowa odpowied藕 modelu.

### Krok 3: Oce j wedug evals

Nadaj wynik (np. Fidelity: 5, Relevance: 5, Safety: 5, Tone: 4) i zapisz to w golden secie.

### Krok 4: Dopisz komentarz wyjaniajcy

Np.: "Odpowied藕 jest poprawna, bo zawiera X, Y i Z. Dopuszczalne wersje: skr贸cona lub rozbudowana, pod warunkiem zachowania merytoryki".

### Krok 5: Wykonaj testy modelu na golden secie

To jest ju偶 Tw贸j pipeline:

1. Model generuje odpowied藕.
2. **Element oceniajcy** (Ty lub automat) sprawdza odpowied藕 wedug Twoich evals.
3. Por贸wnujemy wyniki z celami zdefiniowanymi w golden secie.
4. Raportujemy jako.

W si贸dmym odcinku tej serii poka偶 Ci, jak zaprz do tego zadania inny model (**LLM-as-a-judge**), 偶eby cakowicie zautomatyzowa ten proces. Ale fundamentem zawsze jest Tw贸j Golden Set.

### Jak wyglda taki proces w praktyce?

- **PROMPT** -> **MODEL** -> **ODPOWIED殴**
- **EVALS** (Twoje kryteria oceny)
- **PORWNANIE** z GOLDEN SETEM (wzorzec + tolerancja)
- **WYNIK TESTU**

Efekt kocowy? Nie dostajesz komunikatu "expected output mismatch", tylko konkretny wniosek: "Model 1.4 spad z Fidelity 4.8 na 3.1". To s testy, kt贸re maj sens.

## Przykad prostego golden case

**Prompt:**
"Podaj 3 zalety test贸w automatycznych."

**Golden answer (wzorzec referencyjny):**

1. Szybki feedback w procesie CI/CD (skr贸cenie regresji).
2. Wy偶sza powtarzalno i eliminacja bd贸w ludzkich przy nudnych zadaniach.
3. Obni偶enie koszt贸w wykrycia bdu dziki przesuniciu test贸w "w lewo" (Shift Left).

**Kryteria oceny (Evals):**

- **Fidelity**: Czy podano dokadnie 3 zalety?
- **Relevance**: Czy zalety faktycznie dotycz test贸w automatycznych?
- **Tone**: Czy odpowied藕 jest profesjonalna i rzeczowa?

**Komentarz QA (Expertise Capture):**
"Zale偶y nam na podkreleniu wpywu biznesowego (Shift Left, regresja). Jeli model poda 'oszczdno czasu testera', jest to akceptowalne, ale ni偶ej punktowane ni偶 'skr贸cenie time-to-market'. Zabronione: stwierdzenia, 偶e testy automatyczne zastpuj testowanie eksploracyjne."

## Goldeny nie s po to, 偶eby model uczy si na pami

Czsty bd to traktowanie golden setu jako datasetu treningowego. Golden set to narzdzie QA, nie ML.

Ma by:

- may (50-300 przypadk贸w),
- dobrze opisany,
- stabilny,
- rcznie wypracowany.

Su偶y do testowania, nie do treningu.

## Kiedy golden set zawodzi?

1. Kiedy jest zbyt may - testujesz tylko "happy path".
2. Kiedy jest sztuczny - wymylasz rzeczy, kt贸rych realni userzy nie zadaj.
3. Kiedy nie ma evals - wtedy to tylko zbi贸r "adnych odpowiedzi", kt贸rych nie da si zmierzy.
4. Kiedy go nie aktualizujesz - model si rozwija, a Twoje wzorce zostaj w przeszoci.

Golden set to 偶ywy organizm.

## Checklista dla QA do tego wpisu

- [ ] Czy Tw贸j Golden Set su偶y do wykrywania **regresji merytorycznej**, a nie tylko bd贸w formatowania?
- [ ] Czy udostpniasz judge'owi **odpowied藕 referencyjn**, by m贸g wykona reference-based evaluation?
- [ ] Czy w komentarzach QA zapisae **intencj biznesow**, a nie tylko "poprawny tekst"?
- [ ] Czy Tw贸j zestaw zawiera **edge-case'y** (kr贸tkie, bdne, zoliwe prompty)?
- [ ] Czy Golden Set jest oddzielony od danych u偶ywanych do fine-tuningu modelu?

## Podsumowanie

Golden set to spos贸b QA na wprowadzenie porzdku w wiecie, w kt贸rym odpowied藕 nigdy nie jest identyczna, a ryzyk jest wicej ni偶 w klasycznych API. W poczeniu z evals daje stabilne testy, powtarzalne oceny i realny wpyw QA na jako system贸w opartych o LLM.

To fundament praktycznego testowania AI. Pora bra si do roboty i zacz budowa wasn "zot bibliotek".

Do usyszenia w kolejnym odcinku serii! 
